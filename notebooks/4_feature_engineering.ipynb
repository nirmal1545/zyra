{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb73ead0",
   "metadata": {},
   "source": [
    "# 4. Feature Engineering & Model Preparation\n",
    "\n",
    "This notebook handles feature engineering and prepares the data for model training.\n",
    "\n",
    "## Objectives\n",
    "1. Load processed data and NLP components\n",
    "2. Engineer relevant features\n",
    "3. Handle class imbalance\n",
    "4. Split data for training\n",
    "5. Prepare feature pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e65820",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7824d805",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09401229",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_and_components():\n",
    "    \"\"\"Load processed data and NLP components.\"\"\"\n",
    "    # Load processed dataset\n",
    "    df = pd.read_csv('../data/processed/processed_data.csv')\n",
    "    \n",
    "    # Load symptom vocabulary\n",
    "    with open('../data/processed/symptom_vocab.json', 'r') as f:\n",
    "        symptom_vocab = json.load(f)\n",
    "    \n",
    "    # Load symptom embeddings\n",
    "    symptom_embeddings = np.load('../data/processed/symptom_embeddings.npy')\n",
    "    \n",
    "    return df, symptom_vocab, symptom_embeddings\n",
    "\n",
    "# Load components\n",
    "df, symptom_vocab, symptom_embeddings = load_data_and_components()\n",
    "\n",
    "print(f\"Loaded dataset with {len(df)} samples\")\n",
    "print(f\"Number of symptoms: {len(symptom_vocab)}\")\n",
    "print(f\"Embedding dimension: {symptom_embeddings.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1845a0",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd73f945",
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(df, symptom_vocab, symptom_embeddings):\n",
    "    \"\"\"Engineer features from symptoms.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Processed dataset\n",
    "        symptom_vocab (dict): Symptom vocabulary\n",
    "        symptom_embeddings (np.array): Pre-computed symptom embeddings\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (feature matrix X, target vector y, feature names)\n",
    "    \"\"\"\n",
    "    # Split features and target\n",
    "    X = df.drop('disease', axis=1)\n",
    "    y = df['disease']\n",
    "    \n",
    "    # Get feature names\n",
    "    feature_names = X.columns.tolist()\n",
    "    \n",
    "    # Add derived features\n",
    "    \n",
    "    # 1. Symptom count per sample\n",
    "    X['symptom_count'] = X.sum(axis=1)\n",
    "    feature_names.append('symptom_count')\n",
    "    \n",
    "    # 2. Weighted symptoms by frequency\n",
    "    weights = np.array([symptom_vocab[s]['frequency'] for s in X.columns[:-1]])\n",
    "    weights = weights / weights.sum()\n",
    "    X['weighted_symptoms'] = (X.iloc[:, :-1] * weights).sum(axis=1)\n",
    "    feature_names.append('weighted_symptoms')\n",
    "    \n",
    "    # 3. Symptom embedding aggregation\n",
    "    agg_embeddings = np.zeros((len(X), symptom_embeddings.shape[1]))\n",
    "    \n",
    "    for i, row in enumerate(X.iloc[:, :-2].values):\n",
    "        active_embeddings = symptom_embeddings[row.astype(bool)]\n",
    "        if len(active_embeddings) > 0:\n",
    "            agg_embeddings[i] = active_embeddings.mean(axis=0)\n",
    "    \n",
    "    # Add embedding features\n",
    "    for i in range(symptom_embeddings.shape[1]):\n",
    "        X[f'emb_{i}'] = agg_embeddings[:, i]\n",
    "        feature_names.append(f'emb_{i}')\n",
    "    \n",
    "    return X.values, y.values, feature_names\n",
    "\n",
    "# Engineer features\n",
    "X, y, feature_names = engineer_features(df, symptom_vocab, symptom_embeddings)\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Number of feature types:\")\n",
    "print(f\"- Binary symptoms: {len(symptom_vocab)}\")\n",
    "print(f\"- Derived features: 2\")\n",
    "print(f\"- Embedding features: {symptom_embeddings.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c3d883",
   "metadata": {},
   "source": [
    "## 3. Handle Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456e04c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_class_distribution(y):\n",
    "    \"\"\"Analyze and visualize class distribution.\"\"\"\n",
    "    print(\"Class Distribution:\")\n",
    "    counts = Counter(y)\n",
    "    for label, count in counts.most_common():\n",
    "        print(f\"- {label}: {count} ({count/len(y)*100:.1f}%)\")\n",
    "\n",
    "print(\"Before SMOTE:\")\n",
    "analyze_class_distribution(y)\n",
    "\n",
    "# Apply SMOTE to balance classes\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "print(\"\\nAfter SMOTE:\")\n",
    "analyze_class_distribution(y_resampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccbb7de",
   "metadata": {},
   "source": [
    "## 4. Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70aaa20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_val_test(X, y, test_size=0.2, val_size=0.2):\n",
    "    \"\"\"Split data into train, validation, and test sets.\n",
    "    \n",
    "    Args:\n",
    "        X (np.array): Feature matrix\n",
    "        y (np.array): Target vector\n",
    "        test_size (float): Proportion for test set\n",
    "        val_size (float): Proportion for validation set\n",
    "        \n",
    "    Returns:\n",
    "        tuple: Train, validation, and test sets\n",
    "    \"\"\"\n",
    "    # First split: separate test set\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Second split: separate train and validation\n",
    "    val_size_adjusted = val_size / (1 - test_size)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_val, y_train_val, \n",
    "        test_size=val_size_adjusted,\n",
    "        random_state=42,\n",
    "        stratify=y_train_val\n",
    "    )\n",
    "    \n",
    "    return (\n",
    "        (X_train, y_train),\n",
    "        (X_val, y_val),\n",
    "        (X_test, y_test)\n",
    "    )\n",
    "\n",
    "# Split data\n",
    "(X_train, y_train), (X_val, y_val), (X_test, y_test) = \\\n",
    "    prepare_train_val_test(X_resampled, y_resampled)\n",
    "\n",
    "print(\"Data split sizes:\")\n",
    "print(f\"Train: {len(X_train)} samples\")\n",
    "print(f\"Validation: {len(X_val)} samples\")\n",
    "print(f\"Test: {len(X_test)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c0042b",
   "metadata": {},
   "source": [
    "## 5. Save Prepared Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076eec8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_prepared_data():\n",
    "    \"\"\"Save prepared datasets and metadata.\"\"\"\n",
    "    # Save splits\n",
    "    np.save('../data/processed/X_train.npy', X_train)\n",
    "    np.save('../data/processed/y_train.npy', y_train)\n",
    "    np.save('../data/processed/X_val.npy', X_val)\n",
    "    np.save('../data/processed/y_val.npy', y_val)\n",
    "    np.save('../data/processed/X_test.npy', X_test)\n",
    "    np.save('../data/processed/y_test.npy', y_test)\n",
    "    \n",
    "    # Save feature names\n",
    "    with open('../data/processed/feature_names.json', 'w') as f:\n",
    "        json.dump(feature_names, f, indent=2)\n",
    "    \n",
    "    print(\"Saved prepared data:\")\n",
    "    print(\"- Train, validation, and test splits\")\n",
    "    print(\"- Feature names\")\n",
    "\n",
    "save_prepared_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdfef04",
   "metadata": {},
   "source": [
    "## 6. Validate Feature Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208c20dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_pipeline():\n",
    "    \"\"\"Validate the feature engineering pipeline.\"\"\"\n",
    "    # Check feature statistics\n",
    "    print(\"Feature Statistics:\")\n",
    "    print(f\"Mean values: {X_train.mean(axis=0)[:5]}...\")\n",
    "    print(f\"Std values: {X_train.std(axis=0)[:5]}...\")\n",
    "    \n",
    "    # Verify class balance\n",
    "    print(\"\\nClass balance in splits:\")\n",
    "    for name, y_split in [('Train', y_train), ('Val', y_val), ('Test', y_test)]:\n",
    "        counts = Counter(y_split)\n",
    "        print(f\"\\n{name} set:\")\n",
    "        for label, count in counts.most_common(5):\n",
    "            print(f\"- {label}: {count}\")\n",
    "    \n",
    "    # Verify feature correlations\n",
    "    print(\"\\nFeature correlations (sample):\")\n",
    "    correlations = np.corrcoef(X_train[:, :5].T)\n",
    "    print(correlations)\n",
    "\n",
    "validate_pipeline()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
