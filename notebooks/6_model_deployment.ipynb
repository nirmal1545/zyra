{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3c60c09",
   "metadata": {},
   "source": [
    "# 6. Model Deployment & API\n",
    "\n",
    "This notebook implements the model deployment and API for symptom-based disease prediction.\n",
    "\n",
    "## Objectives\n",
    "1. Load trained model and components\n",
    "2. Implement prediction pipeline\n",
    "3. Create FastAPI application\n",
    "4. Test API endpoints\n",
    "5. Generate API documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f2e009",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "import uvicorn\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import List, Dict\n",
    "import spacy\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "from rapidfuzz import fuzz\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae18212f",
   "metadata": {},
   "source": [
    "## 1. Load Model & Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571ed392",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_components():\n",
    "    \"\"\"Load all required components for prediction.\"\"\"\n",
    "    # Load model and metadata\n",
    "    model = joblib.load('../models/best_model.joblib')\n",
    "    \n",
    "    with open('../models/model_metadata.json', 'r') as f:\n",
    "        model_metadata = json.load(f)\n",
    "    \n",
    "    # Load NLP components\n",
    "    with open('../data/processed/symptom_vocab.json', 'r') as f:\n",
    "        symptom_vocab = json.load(f)\n",
    "        \n",
    "    symptom_embeddings = np.load('../data/processed/symptom_embeddings.npy')\n",
    "    \n",
    "    # Load NLP models\n",
    "    nlp = spacy.load('en_core_web_md')\n",
    "    model_st = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'metadata': model_metadata,\n",
    "        'symptom_vocab': symptom_vocab,\n",
    "        'symptom_embeddings': symptom_embeddings,\n",
    "        'nlp': nlp,\n",
    "        'sentence_transformer': model_st\n",
    "    }\n",
    "\n",
    "# Load all components\n",
    "components = load_components()\n",
    "print(\"Loaded components:\")\n",
    "for name in components:\n",
    "    print(f\"- {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270554d5",
   "metadata": {},
   "source": [
    "## 2. Implement Prediction Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcd4f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SymptomPredictor:\n",
    "    def __init__(self, components):\n",
    "        \"\"\"Initialize predictor with components.\"\"\"\n",
    "        self.model = components['model']\n",
    "        self.metadata = components['metadata']\n",
    "        self.symptom_vocab = components['symptom_vocab']\n",
    "        self.symptom_embeddings = components['symptom_embeddings']\n",
    "        self.nlp = components['nlp']\n",
    "        self.sentence_transformer = components['sentence_transformer']\n",
    "        \n",
    "        # Initialize spell checker\n",
    "        self.spell = SymSpell(max_dictionary_edit_distance=2)\n",
    "        for symptom in self.symptom_vocab:\n",
    "            clean_name = self.symptom_vocab[symptom]['clean_name']\n",
    "            for word in clean_name.split():\n",
    "                self.spell.create_dictionary_entry(word, 1)\n",
    "    \n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"Preprocess input text.\"\"\"\n",
    "        # Lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Process with spaCy\n",
    "        doc = self.nlp(text)\n",
    "        \n",
    "        # Tokenize and lemmatize\n",
    "        tokens = [token.lemma_ for token in doc \n",
    "                 if not token.is_stop and not token.is_punct]\n",
    "        \n",
    "        return ' '.join(tokens)\n",
    "    \n",
    "    def correct_spelling(self, text):\n",
    "        \"\"\"Correct spelling in text.\"\"\"\n",
    "        words = text.split()\n",
    "        corrected = []\n",
    "        \n",
    "        for word in words:\n",
    "            suggestions = self.spell.lookup(word, Verbosity.CLOSEST, max_edit_distance=2)\n",
    "            if suggestions:\n",
    "                corrected.append(suggestions[0].term)\n",
    "            else:\n",
    "                corrected.append(word)\n",
    "        \n",
    "        return ' '.join(corrected)\n",
    "    \n",
    "    def extract_symptoms(self, text, threshold_fuzzy=85, threshold_semantic=0.72):\n",
    "        \"\"\"Extract symptoms from text.\"\"\"\n",
    "        matched_symptoms = []\n",
    "        text_embedding = self.sentence_transformer.encode([text])[0]\n",
    "        \n",
    "        # Create binary vector\n",
    "        binary_vector = np.zeros(len(self.symptom_vocab))\n",
    "        \n",
    "        for idx, (symptom, data) in enumerate(self.symptom_vocab.items()):\n",
    "            clean_name = data['clean_name']\n",
    "            \n",
    "            # Try fuzzy matching\n",
    "            fuzzy_score = fuzz.partial_ratio(clean_name, text)\n",
    "            if fuzzy_score >= threshold_fuzzy:\n",
    "                matched_symptoms.append({\n",
    "                    'symptom': symptom,\n",
    "                    'match_type': 'fuzzy',\n",
    "                    'score': fuzzy_score\n",
    "                })\n",
    "                binary_vector[idx] = 1\n",
    "                continue\n",
    "            \n",
    "            # Try semantic matching\n",
    "            symptom_emb = np.array(data['embedding'])\n",
    "            semantic_score = np.dot(text_embedding, symptom_emb)\n",
    "            if semantic_score >= threshold_semantic:\n",
    "                matched_symptoms.append({\n",
    "                    'symptom': symptom,\n",
    "                    'match_type': 'semantic',\n",
    "                    'score': float(semantic_score)\n",
    "                })\n",
    "                binary_vector[idx] = 1\n",
    "        \n",
    "        return matched_symptoms, binary_vector\n",
    "    \n",
    "    def prepare_features(self, binary_vector):\n",
    "        \"\"\"Prepare feature vector for prediction.\"\"\"\n",
    "        # Add derived features as in training\n",
    "        features = list(binary_vector)\n",
    "        \n",
    "        # Symptom count\n",
    "        features.append(binary_vector.sum())\n",
    "        \n",
    "        # Weighted symptoms\n",
    "        weights = np.array([self.symptom_vocab[s]['frequency'] \n",
    "                           for s in self.symptom_vocab])\n",
    "        weights = weights / weights.sum()\n",
    "        features.append(float(np.dot(binary_vector, weights)))\n",
    "        \n",
    "        # Embedding features\n",
    "        active_embeddings = self.symptom_embeddings[binary_vector.astype(bool)]\n",
    "        if len(active_embeddings) > 0:\n",
    "            agg_embedding = active_embeddings.mean(axis=0)\n",
    "        else:\n",
    "            agg_embedding = np.zeros(self.symptom_embeddings.shape[1])\n",
    "        \n",
    "        features.extend(agg_embedding.tolist())\n",
    "        \n",
    "        return np.array(features).reshape(1, -1)\n",
    "    \n",
    "    def predict(self, text):\n",
    "        \"\"\"Make prediction from raw text input.\"\"\"\n",
    "        # Preprocess\n",
    "        processed = self.preprocess_text(text)\n",
    "        corrected = self.correct_spelling(processed)\n",
    "        \n",
    "        # Extract symptoms\n",
    "        matched_symptoms, binary_vector = self.extract_symptoms(corrected)\n",
    "        \n",
    "        if not matched_symptoms:\n",
    "            raise ValueError(\"No symptoms detected in input text\")\n",
    "        \n",
    "        # Prepare features\n",
    "        features = self.prepare_features(binary_vector)\n",
    "        \n",
    "        # Make prediction\n",
    "        disease = self.model.predict(features)[0]\n",
    "        probabilities = self.model.predict_proba(features)[0]\n",
    "        \n",
    "        # Get top 3 predictions\n",
    "        top_indices = np.argsort(probabilities)[-3:][::-1]\n",
    "        predictions = [\n",
    "            {\n",
    "                'disease': self.model.classes_[idx],\n",
    "                'probability': float(probabilities[idx])\n",
    "            }\n",
    "            for idx in top_indices\n",
    "        ]\n",
    "        \n",
    "        return {\n",
    "            'detected_symptoms': matched_symptoms,\n",
    "            'predictions': predictions\n",
    "        }\n",
    "\n",
    "# Initialize predictor\n",
    "predictor = SymptomPredictor(components)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdca3e1",
   "metadata": {},
   "source": [
    "## 3. Create FastAPI Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbc86f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define request/response models\n",
    "class PredictionRequest(BaseModel):\n",
    "    text: str\n",
    "    \n",
    "class SymptomMatch(BaseModel):\n",
    "    symptom: str\n",
    "    match_type: str\n",
    "    score: float\n",
    "    \n",
    "class Prediction(BaseModel):\n",
    "    disease: str\n",
    "    probability: float\n",
    "    \n",
    "class PredictionResponse(BaseModel):\n",
    "    detected_symptoms: List[SymptomMatch]\n",
    "    predictions: List[Prediction]\n",
    "\n",
    "# Create FastAPI app\n",
    "app = FastAPI(\n",
    "    title=\"Disease Prediction API\",\n",
    "    description=\"API for predicting diseases from symptom descriptions\",\n",
    "    version=\"1.0.0\"\n",
    ")\n",
    "\n",
    "@app.post(\"/predict\", response_model=PredictionResponse)\n",
    "async def predict(request: PredictionRequest):\n",
    "    \"\"\"Predict disease from symptom description.\"\"\"\n",
    "    try:\n",
    "        result = predictor.predict(request.text)\n",
    "        return result\n",
    "    except ValueError as e:\n",
    "        raise HTTPException(status_code=400, detail=str(e))\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=\"Internal server error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c07d924",
   "metadata": {},
   "source": [
    "## 4. Test API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafeaa50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_api():\n",
    "    \"\"\"Test API with sample inputs.\"\"\"\n",
    "    test_cases = [\n",
    "        \"I have a severe headache and high fever\",\n",
    "        \"experiencing chest pain and difficulty breathing\",\n",
    "        \"feeling dizzy and nautious with blurred vision\",  # Misspelled\n",
    "        \"\"  # Empty input\n",
    "    ]\n",
    "    \n",
    "    print(\"Testing API endpoints:\\n\")\n",
    "    for text in test_cases:\n",
    "        print(f\"Input: {text}\")\n",
    "        try:\n",
    "            result = predictor.predict(text)\n",
    "            print(\"\\nDetected Symptoms:\")\n",
    "            for symptom in result['detected_symptoms']:\n",
    "                print(f\"- {symptom['symptom']} ({symptom['match_type']}, {symptom['score']:.2f})\")\n",
    "            \n",
    "            print(\"\\nPredictions:\")\n",
    "            for pred in result['predictions']:\n",
    "                print(f\"- {pred['disease']}: {pred['probability']:.2%}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {str(e)}\")\n",
    "        print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "test_api()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbae93cf",
   "metadata": {},
   "source": [
    "## 5. Start API Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cfae53",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Start API server\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
