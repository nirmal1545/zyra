{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "447d51f2",
   "metadata": {},
   "source": [
    "# 1. Data Loading & Preprocessing\n",
    "\n",
    "This notebook handles the initial data loading and preprocessing steps for our symptom-to-disease prediction pipeline.\n",
    "\n",
    "## Objectives\n",
    "1. Load large dataset efficiently using chunking\n",
    "2. Validate data quality\n",
    "3. Normalize disease labels\n",
    "4. Handle multi-label cases\n",
    "5. Save processed dataset\n",
    "\n",
    "## Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec81f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!python -m pip install --upgrade pip\n",
    "!pip install pandas numpy scikit-learn joblib python-dotenv tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e729f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from pathlib import Path\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be59e854",
   "metadata": {},
   "source": [
    "## 1. Data Loading\n",
    "\n",
    "We'll load the data in chunks to handle the large file size efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75232d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_in_chunks(file_path, chunksize=10000):\n",
    "    \"\"\"Load large dataset in chunks and combine.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the data file\n",
    "        chunksize (int): Number of rows per chunk\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Combined dataset\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    total_rows = sum(1 for _ in open(file_path)) - 1  # Subtract header\n",
    "    \n",
    "    with tqdm(total=total_rows, desc=\"Loading data\") as pbar:\n",
    "        for chunk in pd.read_csv(file_path, chunksize=chunksize):\n",
    "            chunks.append(chunk)\n",
    "            pbar.update(len(chunk))\n",
    "    \n",
    "    return pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "# Load the dataset\n",
    "DATA_PATH = \"../data/raw/disease_symptom_dataset.csv\"  # Update with your file path\n",
    "try:\n",
    "    df = load_data_in_chunks(DATA_PATH)\n",
    "    print(f\"Dataset loaded successfully with shape: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Please place the dataset at {DATA_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de4523d",
   "metadata": {},
   "source": [
    "## 2. Data Validation\n",
    "\n",
    "Check for data quality issues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1332153e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_dataset(df):\n",
    "    \"\"\"Perform data validation checks.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataset\n",
    "        \n",
    "    Returns:\n",
    "        dict: Validation results\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'total_rows': len(df),\n",
    "        'duplicates': df.duplicated().sum(),\n",
    "        'missing_values': df.isnull().sum().sum(),\n",
    "        'non_binary_columns': []\n",
    "    }\n",
    "    \n",
    "    # Check for non-binary values in symptom columns\n",
    "    symptom_cols = df.columns[1:]  # All columns except disease\n",
    "    for col in symptom_cols:\n",
    "        unique_vals = df[col].unique()\n",
    "        if not all(val in [0, 1] for val in unique_vals if pd.notna(val)):\n",
    "            results['non_binary_columns'].append(col)\n",
    "    \n",
    "    return results\n",
    "\n",
    "validation_results = validate_dataset(df)\n",
    "print(\"\\nValidation Results:\")\n",
    "print(f\"Total rows: {validation_results['total_rows']}\")\n",
    "print(f\"Duplicate rows: {validation_results['duplicates']}\")\n",
    "print(f\"Missing values: {validation_results['missing_values']}\")\n",
    "print(f\"Columns with non-binary values: {len(validation_results['non_binary_columns'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558d5c46",
   "metadata": {},
   "source": [
    "## 3. Disease Label Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf906c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_disease_labels(df):\n",
    "    \"\"\"Normalize disease labels and save mapping.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataset\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (transformed DataFrame, label encoder)\n",
    "    \"\"\"\n",
    "    le = LabelEncoder()\n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    # Fit and transform disease labels\n",
    "    df_processed['disease'] = le.fit_transform(df['disease'])\n",
    "    \n",
    "    # Save label mapping\n",
    "    label_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "    with open('../data/processed/disease_mapping.json', 'w') as f:\n",
    "        json.dump(label_mapping, f, indent=2)\n",
    "    \n",
    "    print(f\"Normalized {len(label_mapping)} unique disease labels\")\n",
    "    return df_processed, le\n",
    "\n",
    "df_normalized, label_encoder = normalize_disease_labels(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8471bf10",
   "metadata": {},
   "source": [
    "## 4. Handle Multi-label Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1644842d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_multi_label_diseases(df):\n",
    "    \"\"\"Process rows with multiple disease labels.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataset\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Processed dataset\n",
    "    \"\"\"\n",
    "    # Check for multi-label indicators (e.g., commas or semicolons in disease names)\n",
    "    multi_label_rows = df['disease'].str.contains('[,;]', regex=True)\n",
    "    \n",
    "    if multi_label_rows.any():\n",
    "        print(f\"Found {multi_label_rows.sum()} multi-label rows\")\n",
    "        \n",
    "        # Split multi-label rows into separate rows\n",
    "        new_rows = []\n",
    "        for idx, row in df[multi_label_rows].iterrows():\n",
    "            diseases = [d.strip() for d in row['disease'].split(',')]\n",
    "            for disease in diseases:\n",
    "                new_row = row.copy()\n",
    "                new_row['disease'] = disease\n",
    "                new_rows.append(new_row)\n",
    "        \n",
    "        # Replace original multi-label rows with split rows\n",
    "        df = pd.concat([\n",
    "            df[~multi_label_rows],\n",
    "            pd.DataFrame(new_rows)\n",
    "        ], ignore_index=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_processed = handle_multi_label_diseases(df_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3809aa28",
   "metadata": {},
   "source": [
    "## 5. Save Processed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702b0a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed dataset\n",
    "output_path = '../data/processed/processed_data.csv'\n",
    "df_processed.to_csv(output_path, index=False)\n",
    "print(f\"Saved processed dataset to {output_path}\")\n",
    "\n",
    "# Save dataset statistics\n",
    "stats = {\n",
    "    'n_samples': len(df_processed),\n",
    "    'n_features': len(df_processed.columns) - 1,  # Exclude disease column\n",
    "    'n_classes': len(df_processed['disease'].unique()),\n",
    "    'memory_usage': df_processed.memory_usage(deep=True).sum() / 1024**2  # MB\n",
    "}\n",
    "\n",
    "with open('../data/processed/dataset_stats.json', 'w') as f:\n",
    "    json.dump(stats, f, indent=2)\n",
    "\n",
    "print(\"\\nDataset Statistics:\")\n",
    "for k, v in stats.items():\n",
    "    print(f\"{k}: {v}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
