{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39d2a875",
   "metadata": {},
   "source": [
    "# 7. Model Monitoring & Maintenance\n",
    "\n",
    "This notebook implements model monitoring, maintenance, and retraining pipelines.\n",
    "\n",
    "## Objectives\n",
    "1. Monitor model performance\n",
    "2. Track prediction distribution\n",
    "3. Collect user feedback\n",
    "4. Implement retraining pipeline\n",
    "5. Set up monitoring dashboards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b88a704",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf235ff",
   "metadata": {},
   "source": [
    "## 1. Performance Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058b86dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelMonitor:\n",
    "    def __init__(self, log_dir='../logs'):\n",
    "        \"\"\"Initialize model monitor.\"\"\"\n",
    "        self.log_dir = Path(log_dir)\n",
    "        self.log_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Initialize metrics storage\n",
    "        self.metrics_file = self.log_dir / 'metrics.json'\n",
    "        self.predictions_file = self.log_dir / 'predictions.csv'\n",
    "        self.feedback_file = self.log_dir / 'feedback.csv'\n",
    "        \n",
    "        self._init_storage()\n",
    "    \n",
    "    def _init_storage(self):\n",
    "        \"\"\"Initialize storage files if they don't exist.\"\"\"\n",
    "        if not self.metrics_file.exists():\n",
    "            with open(self.metrics_file, 'w') as f:\n",
    "                json.dump({\n",
    "                    'daily_metrics': [],\n",
    "                    'alerts': []\n",
    "                }, f, indent=2)\n",
    "        \n",
    "        if not self.predictions_file.exists():\n",
    "            pd.DataFrame(columns=[\n",
    "                'timestamp', 'input_text', 'detected_symptoms',\n",
    "                'predicted_disease', 'confidence', 'response_time'\n",
    "            ]).to_csv(self.predictions_file, index=False)\n",
    "        \n",
    "        if not self.feedback_file.exists():\n",
    "            pd.DataFrame(columns=[\n",
    "                'timestamp', 'prediction_id', 'actual_disease',\n",
    "                'feedback_text', 'rating'\n",
    "            ]).to_csv(self.feedback_file, index=False)\n",
    "    \n",
    "    def log_prediction(self, prediction_data):\n",
    "        \"\"\"Log a new prediction.\"\"\"\n",
    "        df = pd.DataFrame([{\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'input_text': prediction_data['input_text'],\n",
    "            'detected_symptoms': ','.join(prediction_data['detected_symptoms']),\n",
    "            'predicted_disease': prediction_data['predicted_disease'],\n",
    "            'confidence': prediction_data['confidence'],\n",
    "            'response_time': prediction_data['response_time']\n",
    "        }])\n",
    "        \n",
    "        df.to_csv(self.predictions_file, mode='a', header=False, index=False)\n",
    "    \n",
    "    def log_feedback(self, feedback_data):\n",
    "        \"\"\"Log user feedback.\"\"\"\n",
    "        df = pd.DataFrame([{\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'prediction_id': feedback_data['prediction_id'],\n",
    "            'actual_disease': feedback_data['actual_disease'],\n",
    "            'feedback_text': feedback_data['feedback_text'],\n",
    "            'rating': feedback_data['rating']\n",
    "        }])\n",
    "        \n",
    "        df.to_csv(self.feedback_file, mode='a', header=False, index=False)\n",
    "    \n",
    "    def compute_daily_metrics(self):\n",
    "        \"\"\"Compute daily performance metrics.\"\"\"\n",
    "        # Load predictions and feedback\n",
    "        predictions_df = pd.read_csv(self.predictions_file)\n",
    "        feedback_df = pd.read_csv(self.feedback_file)\n",
    "        \n",
    "        # Convert timestamps\n",
    "        predictions_df['timestamp'] = pd.to_datetime(predictions_df['timestamp'])\n",
    "        feedback_df['timestamp'] = pd.to_datetime(feedback_df['timestamp'])\n",
    "        \n",
    "        # Get today's data\n",
    "        today = datetime.now().date()\n",
    "        today_predictions = predictions_df[\n",
    "            predictions_df['timestamp'].dt.date == today\n",
    "        ]\n",
    "        today_feedback = feedback_df[\n",
    "            feedback_df['timestamp'].dt.date == today\n",
    "        ]\n",
    "        \n",
    "        # Compute metrics\n",
    "        metrics = {\n",
    "            'date': today.isoformat(),\n",
    "            'total_predictions': len(today_predictions),\n",
    "            'avg_confidence': float(today_predictions['confidence'].mean()),\n",
    "            'avg_response_time': float(today_predictions['response_time'].mean()),\n",
    "            'feedback_count': len(today_feedback),\n",
    "            'avg_rating': float(today_feedback['rating'].mean() if len(today_feedback) > 0 else 0)\n",
    "        }\n",
    "        \n",
    "        # Update metrics file\n",
    "        with open(self.metrics_file, 'r+') as f:\n",
    "            data = json.load(f)\n",
    "            data['daily_metrics'].append(metrics)\n",
    "            f.seek(0)\n",
    "            json.dump(data, f, indent=2)\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def check_alerts(self, metrics):\n",
    "        \"\"\"Check for performance alerts.\"\"\"\n",
    "        alerts = []\n",
    "        \n",
    "        # Define thresholds\n",
    "        thresholds = {\n",
    "            'min_confidence': 0.7,\n",
    "            'max_response_time': 1.0,\n",
    "            'min_rating': 3.5\n",
    "        }\n",
    "        \n",
    "        # Check confidence\n",
    "        if metrics['avg_confidence'] < thresholds['min_confidence']:\n",
    "            alerts.append({\n",
    "                'type': 'low_confidence',\n",
    "                'message': f\"Average confidence ({metrics['avg_confidence']:.2f}) below threshold\",\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            })\n",
    "        \n",
    "        # Check response time\n",
    "        if metrics['avg_response_time'] > thresholds['max_response_time']:\n",
    "            alerts.append({\n",
    "                'type': 'high_latency',\n",
    "                'message': f\"Average response time ({metrics['avg_response_time']:.2f}s) above threshold\",\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            })\n",
    "        \n",
    "        # Check user satisfaction\n",
    "        if metrics['feedback_count'] > 0 and metrics['avg_rating'] < thresholds['min_rating']:\n",
    "            alerts.append({\n",
    "                'type': 'low_satisfaction',\n",
    "                'message': f\"Average rating ({metrics['avg_rating']:.2f}) below threshold\",\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            })\n",
    "        \n",
    "        # Update alerts in metrics file\n",
    "        if alerts:\n",
    "            with open(self.metrics_file, 'r+') as f:\n",
    "                data = json.load(f)\n",
    "                data['alerts'].extend(alerts)\n",
    "                f.seek(0)\n",
    "                json.dump(data, f, indent=2)\n",
    "        \n",
    "        return alerts\n",
    "\n",
    "# Initialize monitor\n",
    "monitor = ModelMonitor()\n",
    "\n",
    "# Test logging\n",
    "test_prediction = {\n",
    "    'input_text': \"severe headache and fever\",\n",
    "    'detected_symptoms': ['headache', 'fever'],\n",
    "    'predicted_disease': 'flu',\n",
    "    'confidence': 0.85,\n",
    "    'response_time': 0.2\n",
    "}\n",
    "\n",
    "monitor.log_prediction(test_prediction)\n",
    "\n",
    "test_feedback = {\n",
    "    'prediction_id': '123',\n",
    "    'actual_disease': 'flu',\n",
    "    'feedback_text': \"Accurate prediction\",\n",
    "    'rating': 5\n",
    "}\n",
    "\n",
    "monitor.log_feedback(test_feedback)\n",
    "\n",
    "# Compute metrics\n",
    "metrics = monitor.compute_daily_metrics()\n",
    "print(\"Daily Metrics:\")\n",
    "print(json.dumps(metrics, indent=2))\n",
    "\n",
    "# Check alerts\n",
    "alerts = monitor.check_alerts(metrics)\n",
    "if alerts:\n",
    "    print(\"\\nAlerts:\")\n",
    "    print(json.dumps(alerts, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92a0cad",
   "metadata": {},
   "source": [
    "## 2. Visualization Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2357d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_monitoring_dashboard():\n",
    "    \"\"\"Create monitoring dashboard with plots.\"\"\"\n",
    "    # Load data\n",
    "    predictions_df = pd.read_csv(monitor.predictions_file)\n",
    "    feedback_df = pd.read_csv(monitor.feedback_file)\n",
    "    \n",
    "    with open(monitor.metrics_file, 'r') as f:\n",
    "        metrics_data = json.load(f)\n",
    "    \n",
    "    metrics_df = pd.DataFrame(metrics_data['daily_metrics'])\n",
    "    \n",
    "    # Create dashboard\n",
    "    plt.style.use('seaborn')\n",
    "    fig = plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # 1. Confidence Distribution\n",
    "    plt.subplot(2, 2, 1)\n",
    "    sns.histplot(data=predictions_df, x='confidence', bins=20)\n",
    "    plt.title('Prediction Confidence Distribution')\n",
    "    plt.xlabel('Confidence')\n",
    "    plt.ylabel('Count')\n",
    "    \n",
    "    # 2. Response Time Trend\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(metrics_df['date'], metrics_df['avg_response_time'])\n",
    "    plt.title('Average Response Time Trend')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Response Time (s)')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # 3. User Ratings Distribution\n",
    "    plt.subplot(2, 2, 3)\n",
    "    sns.countplot(data=feedback_df, x='rating')\n",
    "    plt.title('User Ratings Distribution')\n",
    "    plt.xlabel('Rating')\n",
    "    plt.ylabel('Count')\n",
    "    \n",
    "    # 4. Daily Prediction Volume\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(metrics_df['date'], metrics_df['total_predictions'])\n",
    "    plt.title('Daily Prediction Volume')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Number of Predictions')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create dashboard\n",
    "create_monitoring_dashboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd2756f",
   "metadata": {},
   "source": [
    "## 3. Retraining Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956ecd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelRetrainer:\n",
    "    def __init__(self, monitor):\n",
    "        \"\"\"Initialize retrainer with monitor.\"\"\"\n",
    "        self.monitor = monitor\n",
    "        self.model_dir = Path('../models')\n",
    "        \n",
    "    def should_retrain(self, min_feedback=100, max_confidence_drop=0.1):\n",
    "        \"\"\"Check if model should be retrained.\"\"\"\n",
    "        # Load metrics\n",
    "        with open(self.monitor.metrics_file, 'r') as f:\n",
    "            metrics_data = json.load(f)\n",
    "        \n",
    "        metrics_df = pd.DataFrame(metrics_data['daily_metrics'])\n",
    "        \n",
    "        # Check feedback volume\n",
    "        total_feedback = metrics_df['feedback_count'].sum()\n",
    "        if total_feedback < min_feedback:\n",
    "            return False\n",
    "        \n",
    "        # Check confidence trend\n",
    "        recent_confidence = metrics_df['avg_confidence'].tail(7).mean()\n",
    "        baseline_confidence = metrics_df['avg_confidence'].head(7).mean()\n",
    "        \n",
    "        if (baseline_confidence - recent_confidence) > max_confidence_drop:\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def prepare_retraining_data(self):\n",
    "        \"\"\"Prepare data for retraining.\"\"\"\n",
    "        # Load feedback data\n",
    "        feedback_df = pd.read_csv(self.monitor.feedback_file)\n",
    "        predictions_df = pd.read_csv(self.monitor.predictions_file)\n",
    "        \n",
    "        # Merge feedback with predictions\n",
    "        retraining_data = predictions_df.merge(\n",
    "            feedback_df,\n",
    "            left_index=True,\n",
    "            right_on='prediction_id',\n",
    "            how='inner'\n",
    "        )\n",
    "        \n",
    "        # Filter high-quality feedback\n",
    "        retraining_data = retraining_data[retraining_data['rating'] >= 4]\n",
    "        \n",
    "        return retraining_data\n",
    "    \n",
    "    def retrain_model(self):\n",
    "        \"\"\"Retrain model with new data.\"\"\"\n",
    "        print(\"Checking retraining criteria...\")\n",
    "        \n",
    "        if not self.should_retrain():\n",
    "            print(\"Retraining criteria not met\")\n",
    "            return\n",
    "        \n",
    "        print(\"Preparing retraining data...\")\n",
    "        new_data = self.prepare_retraining_data()\n",
    "        \n",
    "        # Load current model\n",
    "        current_model = joblib.load(self.model_dir / 'best_model.joblib')\n",
    "        \n",
    "        print(f\"Starting retraining with {len(new_data)} new samples...\")\n",
    "        \n",
    "        # TODO: Implement actual retraining\n",
    "        # This would involve:\n",
    "        # 1. Combining new data with original training data\n",
    "        # 2. Retraining the model\n",
    "        # 3. Evaluating performance\n",
    "        # 4. Saving if better than current model\n",
    "        \n",
    "        print(\"Retraining complete\")\n",
    "\n",
    "# Initialize retrainer\n",
    "retrainer = ModelRetrainer(monitor)\n",
    "\n",
    "# Check retraining\n",
    "retrainer.retrain_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccbefee",
   "metadata": {},
   "source": [
    "## 4. Schedule Maintenance Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7233404",
   "metadata": {},
   "outputs": [],
   "source": [
    "def schedule_maintenance():\n",
    "    \"\"\"Schedule regular maintenance tasks.\"\"\"\n",
    "    # This would typically be done using a task scheduler like cron\n",
    "    # For demonstration, we'll just list the tasks\n",
    "    \n",
    "    maintenance_schedule = {\n",
    "        'daily': [\n",
    "            'Compute performance metrics',\n",
    "            'Check for alerts',\n",
    "            'Update dashboard'\n",
    "        ],\n",
    "        'weekly': [\n",
    "            'Analyze prediction patterns',\n",
    "            'Review user feedback',\n",
    "            'Check retraining criteria'\n",
    "        ],\n",
    "        'monthly': [\n",
    "            'Full model evaluation',\n",
    "            'Data quality assessment',\n",
    "            'System health check'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    print(\"Maintenance Schedule:\")\n",
    "    for frequency, tasks in maintenance_schedule.items():\n",
    "        print(f\"\\n{frequency.title()} Tasks:\")\n",
    "        for task in tasks:\n",
    "            print(f\"- {task}\")\n",
    "\n",
    "schedule_maintenance()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
