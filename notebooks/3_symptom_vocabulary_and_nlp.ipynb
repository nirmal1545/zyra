{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce3f14d0",
   "metadata": {},
   "source": [
    "# 3. Symptom Vocabulary & NLP Pipeline\n",
    "\n",
    "This notebook implements the NLP pipeline for processing user-reported symptoms.\n",
    "\n",
    "## Objectives\n",
    "1. Build canonical symptom vocabulary\n",
    "2. Implement text preprocessing\n",
    "3. Create spell correction system\n",
    "4. Implement symptom extraction\n",
    "5. Save vocabulary and pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33067342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install spacy symspellpy rapidfuzz sentence-transformers pandas numpy\n",
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff0c0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import json\n",
    "from pathlib import Path\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "from rapidfuzz import fuzz\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "# Initialize sentence transformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7892667e",
   "metadata": {},
   "source": [
    "## 1. Build Symptom Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe1235c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_symptom_vocabulary(df):\n",
    "    \"\"\"Create canonical symptom vocabulary from dataset columns.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Processed dataset\n",
    "        \n",
    "    Returns:\n",
    "        dict: Symptom vocabulary with metadata\n",
    "    \"\"\"\n",
    "    # Get symptom columns (all except 'disease')\n",
    "    symptom_cols = df.columns[1:].tolist()\n",
    "    \n",
    "    # Process symptom names\n",
    "    vocab = {}\n",
    "    for symptom in symptom_cols:\n",
    "        # Clean symptom name\n",
    "        clean_name = symptom.lower().replace('_', ' ')\n",
    "        \n",
    "        # Get embedding for semantic matching\n",
    "        embedding = model.encode([clean_name])[0]\n",
    "        \n",
    "        # Store in vocabulary\n",
    "        vocab[symptom] = {\n",
    "            'clean_name': clean_name,\n",
    "            'embedding': embedding.tolist(),\n",
    "            'frequency': int(df[symptom].sum()),\n",
    "            'alternatives': []  # Will be filled with common variations\n",
    "        }\n",
    "    \n",
    "    return vocab\n",
    "\n",
    "# Load processed data and build vocabulary\n",
    "df = pd.read_csv('../data/processed/processed_data.csv')\n",
    "symptom_vocab = build_symptom_vocabulary(df)\n",
    "\n",
    "print(f\"Built vocabulary with {len(symptom_vocab)} symptoms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da7630f",
   "metadata": {},
   "source": [
    "## 2. Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794fd829",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"Preprocess user input text.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Raw user input\n",
    "        \n",
    "    Returns:\n",
    "        str: Preprocessed text\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Process with spaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Tokenize and lemmatize\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Test preprocessing\n",
    "test_text = \"I've been having severe headaches and feeling nauseous for the past few days\"\n",
    "print(f\"Original: {test_text}\")\n",
    "print(f\"Preprocessed: {preprocess_text(test_text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c94887c",
   "metadata": {},
   "source": [
    "## 3. Spell Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b179aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_spell_checker():\n",
    "    \"\"\"Initialize SymSpell for spell correction.\"\"\"\n",
    "    spell = SymSpell(max_dictionary_edit_distance=2)\n",
    "    \n",
    "    # Add symptom vocabulary to dictionary\n",
    "    for symptom in symptom_vocab.values():\n",
    "        clean_name = symptom['clean_name']\n",
    "        words = clean_name.split()\n",
    "        \n",
    "        # Add individual words and full phrase\n",
    "        for word in words:\n",
    "            spell.create_dictionary_entry(word, 1)\n",
    "        spell.create_dictionary_entry(clean_name, 1)\n",
    "    \n",
    "    return spell\n",
    "\n",
    "def correct_spelling(text, spell_checker):\n",
    "    \"\"\"Correct spelling in text.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text\n",
    "        spell_checker: SymSpell instance\n",
    "        \n",
    "    Returns:\n",
    "        str: Text with corrected spelling\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    corrected = []\n",
    "    \n",
    "    for word in words:\n",
    "        suggestions = spell_checker.lookup(word, Verbosity.CLOSEST, max_edit_distance=2)\n",
    "        if suggestions:\n",
    "            corrected.append(suggestions[0].term)\n",
    "        else:\n",
    "            corrected.append(word)\n",
    "    \n",
    "    return ' '.join(corrected)\n",
    "\n",
    "# Initialize spell checker\n",
    "spell_checker = initialize_spell_checker()\n",
    "\n",
    "# Test spell correction\n",
    "test_text = \"i have hedache and fver\"\n",
    "corrected = correct_spelling(test_text, spell_checker)\n",
    "print(f\"Original: {test_text}\")\n",
    "print(f\"Corrected: {corrected}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318b2d5b",
   "metadata": {},
   "source": [
    "## 4. Symptom Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f1f91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_symptoms(text, threshold_fuzzy=85, threshold_semantic=0.72):\n",
    "    \"\"\"Extract symptoms from text using fuzzy and semantic matching.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Preprocessed input text\n",
    "        threshold_fuzzy (int): Threshold for fuzzy matching\n",
    "        threshold_semantic (float): Threshold for semantic similarity\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (matched symptoms, binary vector)\n",
    "    \"\"\"\n",
    "    matched_symptoms = []\n",
    "    text_embedding = model.encode([text])[0]\n",
    "    \n",
    "    # Create binary vector\n",
    "    binary_vector = np.zeros(len(symptom_vocab))\n",
    "    \n",
    "    for idx, (symptom, data) in enumerate(symptom_vocab.items()):\n",
    "        clean_name = data['clean_name']\n",
    "        \n",
    "        # Try fuzzy matching first\n",
    "        fuzzy_score = fuzz.partial_ratio(clean_name, text)\n",
    "        if fuzzy_score >= threshold_fuzzy:\n",
    "            matched_symptoms.append((symptom, 'fuzzy', fuzzy_score))\n",
    "            binary_vector[idx] = 1\n",
    "            continue\n",
    "        \n",
    "        # Try semantic matching\n",
    "        semantic_score = np.dot(text_embedding, data['embedding'])\n",
    "        if semantic_score >= threshold_semantic:\n",
    "            matched_symptoms.append((symptom, 'semantic', semantic_score))\n",
    "            binary_vector[idx] = 1\n",
    "    \n",
    "    return matched_symptoms, binary_vector\n",
    "\n",
    "# Test symptom extraction\n",
    "test_input = \"I have a severe headache and high fever, feeling very weak\"\n",
    "\n",
    "# Preprocess\n",
    "processed = preprocess_text(test_input)\n",
    "corrected = correct_spelling(processed, spell_checker)\n",
    "\n",
    "# Extract symptoms\n",
    "matched, vector = extract_symptoms(corrected)\n",
    "\n",
    "print(\"Input:\", test_input)\n",
    "print(\"\\nMatched Symptoms:\")\n",
    "for symptom, method, score in matched:\n",
    "    print(f\"- {symptom} ({method}, score: {score:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6a778c",
   "metadata": {},
   "source": [
    "## 5. Save Vocabulary and Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b16aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_nlp_pipeline():\n",
    "    \"\"\"Save vocabulary and pipeline components.\"\"\"\n",
    "    # Save symptom vocabulary (without embeddings for JSON compatibility)\n",
    "    vocab_json = {}\n",
    "    for symptom, data in symptom_vocab.items():\n",
    "        vocab_json[symptom] = {\n",
    "            'clean_name': data['clean_name'],\n",
    "            'frequency': data['frequency'],\n",
    "            'alternatives': data['alternatives']\n",
    "        }\n",
    "    \n",
    "    with open('../data/processed/symptom_vocab.json', 'w') as f:\n",
    "        json.dump(vocab_json, f, indent=2)\n",
    "    \n",
    "    # Save embeddings separately (as numpy array)\n",
    "    embeddings = np.array([data['embedding'] for data in symptom_vocab.values()])\n",
    "    np.save('../data/processed/symptom_embeddings.npy', embeddings)\n",
    "    \n",
    "    print(\"Saved NLP pipeline components:\")\n",
    "    print(\"- Symptom vocabulary: symptom_vocab.json\")\n",
    "    print(\"- Symptom embeddings: symptom_embeddings.npy\")\n",
    "\n",
    "save_nlp_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0514be",
   "metadata": {},
   "source": [
    "## 6. Pipeline Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1071fdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_pipeline():\n",
    "    \"\"\"Test pipeline with various example inputs.\"\"\"\n",
    "    test_cases = [\n",
    "        \"I have a terrible headache and fever\",\n",
    "        \"feeling dizzy and nautious\",  # Misspelled\n",
    "        \"my throat is sore and i'm coughing a lot\",\n",
    "        \"experiencing chest pain and shortness of breath\",\n",
    "        \"\"  # Empty input\n",
    "    ]\n",
    "    \n",
    "    print(\"Testing NLP Pipeline:\\n\")\n",
    "    \n",
    "    for text in test_cases:\n",
    "        print(f\"Input: {text}\")\n",
    "        \n",
    "        if not text:\n",
    "            print(\"↳ No symptoms provided\\n\")\n",
    "            continue\n",
    "            \n",
    "        # Process text\n",
    "        processed = preprocess_text(text)\n",
    "        corrected = correct_spelling(processed, spell_checker)\n",
    "        matched, vector = extract_symptoms(corrected)\n",
    "        \n",
    "        print(f\"↳ Corrected: {corrected}\")\n",
    "        print(\"↳ Matched Symptoms:\")\n",
    "        for symptom, method, score in matched:\n",
    "            print(f\"  - {symptom} ({method}, {score:.2f})\")\n",
    "        print()\n",
    "\n",
    "validate_pipeline()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
